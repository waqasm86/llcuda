{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLaMA CUDA - Kaggle/Colab Example\n",
    "\n",
    "This notebook demonstrates how to use the `llcuda` Python package for CUDA-accelerated LLM inference on NVIDIA T4 GPUs (Kaggle/Colab).\n",
    "\n",
    "**GPU Required:** This notebook requires a GPU runtime (T4, P100, or V100)\n",
    "\n",
    "**Setup:**\n",
    "1. Enable GPU runtime: Runtime → Change runtime type → GPU\n",
    "2. Install the package\n",
    "3. Download a model\n",
    "4. Run inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NVIDIA GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install llcuda Package\n",
    "\n",
    "### Option A: Install from PyPI (when published)\n",
    "```python\n",
    "!pip install llcuda\n",
    "```\n",
    "\n",
    "### Option B: Install from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install pybind11 numpy\n",
    "\n",
    "# Clone and install from source\n",
    "!git clone https://github.com/waqasm86/local-llama-cuda.git\n",
    "%cd local-llama-cuda\n",
    "\n",
    "# Set CUDA architecture for T4 GPU\n",
    "import os\n",
    "os.environ['CUDA_ARCHITECTURES'] = '75'  # T4 GPU compute capability\n",
    "\n",
    "# Install\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install llama.cpp Server\n",
    "\n",
    "We need llama-server running as the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git /content/llama.cpp\n",
    "%cd /content/llama.cpp\n",
    "\n",
    "# Build with CUDA support\n",
    "!mkdir -p build && cd build && cmake .. -DGGML_CUDA=ON && cmake --build . --config Release -j8\n",
    "\n",
    "# Verify build\n",
    "!./build/bin/llama-server --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download a Model\n",
    "\n",
    "Download a small GGUF model for testing. We'll use Gemma 2B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model (using HuggingFace)\n",
    "!pip install huggingface_hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "    filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "    local_dir=\"/content/models\"\n",
    ")\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Start llama-server (Background Process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Start llama-server in background\n",
    "server_process = subprocess.Popen([\n",
    "    '/content/llama.cpp/build/bin/llama-server',\n",
    "    '-m', model_path,\n",
    "    '--port', '8090',\n",
    "    '-ngl', '99',  # Offload all layers to GPU\n",
    "    '-c', '4096',  # Context size\n",
    "    '-b', '512',   # Batch size\n",
    "], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Wait for server to start\n",
    "print(\"Starting llama-server...\")\n",
    "for i in range(30):\n",
    "    try:\n",
    "        response = requests.get('http://127.0.0.1:8090/health', timeout=1)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Server is ready!\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        print(f\"Waiting... {i+1}/30\")\n",
    "else:\n",
    "    print(\"❌ Server failed to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Basic Inference with llcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llcuda\n",
    "\n",
    "# Check CUDA availability\n",
    "print(\"CUDA Available:\", llcuda.check_cuda_available())\n",
    "print(\"GPU Info:\", llcuda.get_cuda_device_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference engine\n",
    "engine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# Load model (dummy file since we're using llama-server backend)\n",
    "import os\n",
    "os.makedirs('/tmp', exist_ok=True)\n",
    "open('/tmp/model.gguf', 'a').close()  # Create dummy file\n",
    "\n",
    "engine.load_model('/tmp/model.gguf', gpu_layers=99)\n",
    "print(\"Model loaded:\", engine.is_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "result = engine.infer(\n",
    "    prompt=\"Explain quantum computing in simple terms.\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED TEXT:\")\n",
    "print(\"=\"*60)\n",
    "print(result.text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Tokens: {result.tokens_generated}\")\n",
    "print(f\"Latency: {result.latency_ms:.2f} ms\")\n",
    "print(f\"Throughput: {result.tokens_per_sec:.2f} tokens/sec\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming inference with callback\n",
    "def print_chunk(chunk):\n",
    "    print(chunk, end='', flush=True)\n",
    "\n",
    "print(\"\\nStreaming output:\")\n",
    "print(\"-\" * 60)\n",
    "result = engine.infer_stream(\n",
    "    prompt=\"Write a haiku about AI.\",\n",
    "    callback=print_chunk,\n",
    "    max_tokens=50\n",
    ")\n",
    "print(\"\\n\" + \"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing\n",
    "prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks.\",\n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "results = engine.batch_infer(prompts, max_tokens=50)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt {i+1}: {prompts[i]}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(result.text)\n",
    "    print(f\"Latency: {result.latency_ms:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Reset metrics\n",
    "engine.reset_metrics()\n",
    "\n",
    "# Run benchmark\n",
    "num_iterations = 20\n",
    "latencies = []\n",
    "\n",
    "print(f\"Running {num_iterations} iterations...\")\n",
    "for i in range(num_iterations):\n",
    "    result = engine.infer(\n",
    "        prompt=\"Hello, how are you?\",\n",
    "        max_tokens=64\n",
    "    )\n",
    "    latencies.append(result.latency_ms)\n",
    "    print(f\"Iteration {i+1}/{num_iterations}: {result.latency_ms:.2f}ms\", end='\\r')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Iterations: {num_iterations}\")\n",
    "print(f\"Mean latency: {np.mean(latencies):.2f} ms\")\n",
    "print(f\"p50: {np.percentile(latencies, 50):.2f} ms\")\n",
    "print(f\"p95: {np.percentile(latencies, 95):.2f} ms\")\n",
    "print(f\"p99: {np.percentile(latencies, 99):.2f} ms\")\n",
    "print(f\"Min: {np.min(latencies):.2f} ms\")\n",
    "print(f\"Max: {np.max(latencies):.2f} ms\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Get System Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics\n",
    "metrics = engine.get_metrics()\n",
    "\n",
    "print(\"\\nLatency Metrics:\")\n",
    "print(f\"  Mean: {metrics['latency']['mean_ms']:.2f} ms\")\n",
    "print(f\"  p50: {metrics['latency']['p50_ms']:.2f} ms\")\n",
    "print(f\"  p95: {metrics['latency']['p95_ms']:.2f} ms\")\n",
    "print(f\"  p99: {metrics['latency']['p99_ms']:.2f} ms\")\n",
    "\n",
    "print(\"\\nThroughput Metrics:\")\n",
    "print(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n",
    "print(f\"  Total requests: {metrics['throughput']['total_requests']}\")\n",
    "print(f\"  Tokens/sec: {metrics['throughput']['tokens_per_sec']:.2f}\")\n",
    "print(f\"  Requests/sec: {metrics['throughput']['requests_per_sec']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop server\n",
    "server_process.terminate()\n",
    "server_process.wait()\n",
    "print(\"Server stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully run CUDA-accelerated LLM inference using the `llcuda` package!\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ CUDA acceleration on T4 GPU\n",
    "- ✅ Simple Python API\n",
    "- ✅ Streaming support\n",
    "- ✅ Batch processing\n",
    "- ✅ Performance metrics\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different models (Llama, Mistral, Phi)\n",
    "- Experiment with different parameters (temperature, top_p)\n",
    "- Benchmark on different GPU types (P100, V100)\n",
    "- Build applications using the API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
