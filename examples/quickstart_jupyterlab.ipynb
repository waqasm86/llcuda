{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llcuda Quick Start Guide - JupyterLab Edition\n",
    "\n",
    "This notebook demonstrates how to use **llcuda** for CUDA-accelerated LLM inference in JupyterLab.\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.11+\n",
    "- NVIDIA GPU with CUDA support\n",
    "- llcuda package installed (`pip install llcuda`)\n",
    "- llama-cpp-cuda binaries in `/media/waqasm86/External1/Project-Nvidia/llama-cpp-cuda/`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and System Check\n",
    "\n",
    "First, let's verify that everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llcuda\n",
    "import sys\n",
    "\n",
    "print(f\"llcuda version: {llcuda.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Print comprehensive system information\n",
    "llcuda.print_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if llcuda.check_cuda_available():\n",
    "    print(\"‚úì CUDA is available!\")\n",
    "    \n",
    "    # Get GPU information\n",
    "    gpu_info = llcuda.get_cuda_device_info()\n",
    "    if gpu_info:\n",
    "        print(f\"\\nCUDA Version: {gpu_info['cuda_version']}\")\n",
    "        print(f\"Number of GPUs: {len(gpu_info['gpus'])}\")\n",
    "        \n",
    "        for i, gpu in enumerate(gpu_info['gpus']):\n",
    "            print(f\"\\nGPU {i}:\")\n",
    "            print(f\"  Name: {gpu['name']}\")\n",
    "            print(f\"  Memory: {gpu['memory']}\")\n",
    "            print(f\"  Driver: {gpu['driver_version']}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available. Please check your NVIDIA drivers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find Available Models\n",
    "\n",
    "Let's find GGUF models in common locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find GGUF models\n",
    "models = llcuda.find_gguf_models()\n",
    "\n",
    "print(f\"Found {len(models)} GGUF models:\\n\")\n",
    "for i, model in enumerate(models):\n",
    "    size_mb = model.stat().st_size / (1024 * 1024)\n",
    "    print(f\"{i+1}. {model.name}\")\n",
    "    print(f\"   Path: {model}\")\n",
    "    print(f\"   Size: {size_mb:.1f} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Usage: Auto-Start Mode (Easiest)\n",
    "\n",
    "This is the simplest way to use llcuda. The package will automatically:\n",
    "1. Find llama-server executable\n",
    "2. Start the server with your model\n",
    "3. Connect and run inference\n",
    "4. Clean up when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your model path here\n",
    "MODEL_PATH = \"/media/waqasm86/External1/Project-Nvidia/llama-cpp-cuda/bin/gemma-3-1b-it-Q4_K_M.gguf\"\n",
    "\n",
    "# Create inference engine\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "# Load model with auto-start\n",
    "# This will automatically start llama-server if it's not running\n",
    "engine.load_model(\n",
    "    model_path=MODEL_PATH,\n",
    "    gpu_layers=20,  # Adjust based on your GPU memory (99 = all layers)\n",
    "    ctx_size=2048,\n",
    "    auto_start=True,  # Auto-start server\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Simple Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "result = engine.infer(\n",
    "    prompt=\"What is artificial intelligence?\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Display results\n",
    "if result.success:\n",
    "    print(\"Generated Text:\")\n",
    "    print(\"=\"*60)\n",
    "    print(result.text)\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Tokens Generated: {result.tokens_generated}\")\n",
    "    print(f\"  Latency: {result.latency_ms:.2f} ms\")\n",
    "    print(f\"  Throughput: {result.tokens_per_sec:.2f} tokens/sec\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try Different Prompts\n",
    "\n",
    "Let's try a few different prompts to see how the model responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about CUDA programming.\",\n",
    "    \"What are the benefits of GPU acceleration?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = engine.infer(prompt, max_tokens=80, temperature=0.7)\n",
    "    \n",
    "    if result.success:\n",
    "        print(result.text)\n",
    "        print(f\"\\n‚ö° {result.tokens_per_sec:.1f} tok/s | {result.latency_ms:.0f}ms\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Inference\n",
    "\n",
    "Process multiple prompts in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"What is natural language processing?\"\n",
    "]\n",
    "\n",
    "print(\"Running batch inference...\\n\")\n",
    "results = engine.batch_infer(batch_prompts, max_tokens=50)\n",
    "\n",
    "for i, (prompt, result) in enumerate(zip(batch_prompts, results), 1):\n",
    "    print(f\"{i}. {prompt}\")\n",
    "    if result.success:\n",
    "        print(f\"   ‚Üí {result.text[:100]}...\")\n",
    "        print(f\"   ‚ö° {result.tokens_per_sec:.1f} tok/s\\n\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Error: {result.error_message}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Metrics\n",
    "\n",
    "Get detailed performance statistics for all inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = engine.get_metrics()\n",
    "\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nLatency Statistics:\")\n",
    "latency = metrics['latency']\n",
    "print(f\"  Mean: {latency['mean_ms']:.2f} ms\")\n",
    "print(f\"  p50:  {latency['p50_ms']:.2f} ms\")\n",
    "print(f\"  p95:  {latency['p95_ms']:.2f} ms\")\n",
    "print(f\"  p99:  {latency['p99_ms']:.2f} ms\")\n",
    "print(f\"  Min:  {latency['min_ms']:.2f} ms\")\n",
    "print(f\"  Max:  {latency['max_ms']:.2f} ms\")\n",
    "\n",
    "print(\"\\nThroughput Statistics:\")\n",
    "throughput = metrics['throughput']\n",
    "print(f\"  Total Tokens: {throughput['total_tokens']}\")\n",
    "print(f\"  Total Requests: {throughput['total_requests']}\")\n",
    "print(f\"  Tokens/sec: {throughput['tokens_per_sec']:.2f}\")\n",
    "print(f\"  Requests/sec: {throughput['requests_per_sec']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Performance (Optional)\n",
    "\n",
    "Create a simple plot of latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    latencies = engine._metrics['latencies']\n",
    "    \n",
    "    if latencies:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        # Latency over time\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(latencies, marker='o')\n",
    "        plt.xlabel('Request Number')\n",
    "        plt.ylabel('Latency (ms)')\n",
    "        plt.title('Inference Latency Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Latency distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(latencies, bins=20, edgecolor='black')\n",
    "        plt.xlabel('Latency (ms)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Latency Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No metrics available yet. Run some inferences first.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"matplotlib not installed. Install with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Manual Server Management\n",
    "\n",
    "For more control, you can manually manage the llama-server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llcuda import ServerManager\n",
    "\n",
    "# Create server manager\n",
    "manager = ServerManager()\n",
    "\n",
    "# Get server information\n",
    "info = manager.get_server_info()\n",
    "print(\"Server Info:\")\n",
    "print(f\"  Running: {info['running']}\")\n",
    "print(f\"  URL: {info['url']}\")\n",
    "print(f\"  PID: {info['process_id']}\")\n",
    "print(f\"  Executable: {info['executable']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. One-Liner Quick Inference\n",
    "\n",
    "For quick tests, use the convenience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick one-liner inference (uses existing server)\n",
    "response = llcuda.quick_infer(\n",
    "    prompt=\"Explain GPU computing in one sentence.\",\n",
    "    max_tokens=50,\n",
    "    auto_start=False  # Use existing server\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Temperature Comparison\n",
    "\n",
    "Compare outputs with different temperature settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a creative story about a robot learning to paint.\"\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "\n",
    "print(\"Comparing Different Temperatures\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = engine.infer(\n",
    "        prompt=prompt,\n",
    "        max_tokens=80,\n",
    "        temperature=temp\n",
    "    )\n",
    "    \n",
    "    if result.success:\n",
    "        print(result.text)\n",
    "    else:\n",
    "        print(f\"Error: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup\n",
    "\n",
    "When you're done, stop the server and clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload model and stop server\n",
    "engine.unload_model()\n",
    "print(\"‚úì Server stopped and resources cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Check CUDA availability and GPU info\n",
    "- ‚úÖ Find GGUF models automatically\n",
    "- ‚úÖ Use auto-start mode for easy setup\n",
    "- ‚úÖ Run single and batch inference\n",
    "- ‚úÖ Monitor performance metrics\n",
    "- ‚úÖ Manage the server manually\n",
    "- ‚úÖ Experiment with different parameters\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different models from HuggingFace\n",
    "2. Experiment with different GPU layer configurations\n",
    "3. Build your own applications using llcuda\n",
    "4. Check out the documentation at: https://github.com/waqasm86/llcuda\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Inferencing! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
